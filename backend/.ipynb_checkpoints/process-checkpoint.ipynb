{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import hashlib, binascii\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "import plaid\n",
    "import sys\n",
    "sys.path.insert(1, '../private')\n",
    "import keys as keys\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles():\n",
    "    dlFiles = listdir(\"./download\")\n",
    "    try:\n",
    "        dlFiles.remove('.DS_Store')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    result = []\n",
    "    for file in dlFiles:\n",
    "        fileObject={\"name\":None, \"data\":None}\n",
    "        newData = pd.read_csv(\"./download/\"+file, header=None, names=newColumns)\n",
    "        fileObject[\"name\"] = file.split(\".\")[0]\n",
    "        fileObject[\"data\"] = newData\n",
    "        result.append(fileObject)\n",
    "    return result\n",
    "\n",
    "\n",
    "def getFile(file):\n",
    "    if file == \"data\":\n",
    "        return pd.read_csv(\"./data/data.csv\", header=None, names=oldColumns,index_col=False)\n",
    "    elif file == \"processed\":\n",
    "        return pd.read_csv(\"./data/processed.csv\",index_col=False)\n",
    "    elif file == \"maps\":\n",
    "        return pd.read_csv(\"./data/1to1maps.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"subCategories\":\n",
    "        return pd.read_csv(\"./data/categories.csv\", header=None, names=['item', 'subCategory'])\n",
    "    elif file == \"categories\":\n",
    "        return pd.read_csv(\"./data/breakdown.csv\", header=None, names=['subCategory', 'category'])\n",
    "    \n",
    "def writeFile(file, df):\n",
    "    if file ==\"maps\":\n",
    "        df.to_csv('./data/1to1maps.csv', index=False, header=False)  \n",
    "    elif file==\"subCategories\":\n",
    "        df.to_csv('./data/categories.csv', index=False, header=False)  \n",
    "    elif file ==\"data\":\n",
    "        df.to_csv('./data/data.csv', index=False, header=False)  \n",
    "\n",
    "        \n",
    "\n",
    "def saveDf(df, fileName, path, header = False):\n",
    "    miliTime = int(round(time.time()))\n",
    "    readableTime = datetime.utcfromtimestamp(miliTime).strftime('%Y-%m-%d')\n",
    "    df.to_csv(f'./backup/{fileName}-{readableTime}.csv', index=False, header=header)\n",
    "    df.to_csv(f'./{path}/{fileName}.csv', index=False, header=header)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hash Data\n",
    "\n",
    "oldColumns=['date','item','debit','credit','subCategory','hash', 'account']\n",
    "addedColumns = ['date','item','debit','credit','hash','account']\n",
    "newColumns=['date','item','debit','credit','card']\n",
    "processedColumns=['item','category','subCategory','date','year','month','debit','credit','balance', 'account']\n",
    "\n",
    "def hashit(df):\n",
    "    hashs = []\n",
    "    for index, row in df.iterrows():\n",
    "        h = hashlib.new('ripemd160')\n",
    "        it = str(row['item'])\n",
    "        it2= ' '.join(re.findall(r\"[\\w']+\", it))\n",
    "        h.update(it2.encode())\n",
    "        h.update(str(row['credit']).encode())\n",
    "        h.update(str(row['debit']).encode())\n",
    "        h.update(str(row['date']).encode())\n",
    "        hashed =h.hexdigest()\n",
    "        hashs.append(hashed)\n",
    "    return hashs\n",
    "\n",
    "def testDf(df):\n",
    "    assert df.dtypes['debit'] == 'float64'\n",
    "    assert df.dtypes['credit'] == 'float64'\n",
    "\n",
    "def fixDf(df):\n",
    "    global oldColumns\n",
    "    df['subCategory'].fillna(\"\",inplace=True)\n",
    "    df=df[oldColumns]\n",
    "    return df\n",
    "\n",
    "def findNewItems(old, new, fileName):\n",
    "    new['hash']= hashit(new)\n",
    "    hashfound = []\n",
    "    for index, row in new.iterrows():\n",
    "        hashfound.append(row['hash'] in old['hash'].values)\n",
    "    new['hashfound']=hashfound\n",
    "    \n",
    "#     new.loc[new['hashfound'] == False, 'account'] = fileName;\n",
    "    new['account'] = new['card']\n",
    "    new.loc[new['card']==0, 'account']=fileName;\n",
    "    new.loc[new['card']==None, 'account']=fileName;\n",
    "\n",
    "    newItems = new[new['hashfound'] == False]\n",
    "    print(newItems)\n",
    "    return newItems\n",
    "\n",
    "def convertToJsonArray(df):\n",
    "    columns = df.columns\n",
    "    result = []\n",
    "    for i, row in df.iterrows():\n",
    "        dummy = {}\n",
    "        for column in columns:\n",
    "            dummy[column]=row[column]\n",
    "        result.append(dummy)\n",
    "    return(result)\n",
    "\n",
    "\n",
    "def writeToJson(df):\n",
    "    items = convertToJsonArray(df)\n",
    "    with open('data/data.json', 'w') as jsonFile:\n",
    "        json.dump(items, jsonFile)\n",
    "\n",
    "def listNewItems(files):\n",
    "    global oldColumns\n",
    "    old = getFile('data')\n",
    "    old['subCategory'].fillna(\"\",inplace=True)\n",
    "    old.fillna(value=0,inplace=True)\n",
    "    testDf(old)\n",
    "\n",
    "    combinedAll = old[oldColumns]\n",
    "    combinedNew = pd.DataFrame(columns=oldColumns)\n",
    "\n",
    "    for new in files:\n",
    "        newData = new['data']\n",
    "        newName = new['name']\n",
    "        newData.fillna(value=0,inplace=True)\n",
    "        testDf(newData)\n",
    "\n",
    "        newItems = findNewItems(combinedAll, newData, newName)\n",
    "        newToSave = newItems[addedColumns]\n",
    "\n",
    "        print(f\"{newName} - {len(newToSave)} new items found\")\n",
    "\n",
    "        combinedAll = pd.concat([combinedAll, newToSave])\n",
    "        combinedNew = pd.concat([combinedNew, newToSave])\n",
    "        \n",
    "    combinedNew = fixDf(combinedNew)\n",
    "    return combinedNew\n",
    "\n",
    "# Process Hashed Data\n",
    "def processData(newItems,doAll = False):\n",
    "    if doAll:\n",
    "        data = getFile('data')\n",
    "    else:\n",
    "        newItems.reset_index(inplace=True)\n",
    "        data = newItems.copy()\n",
    "\n",
    "    maps = getFile('maps')\n",
    "    subCategories = getFile('subCategories')\n",
    "    categories = getFile('categories')\n",
    "\n",
    "    categoryMap ={}\n",
    "    for i, row in categories.iterrows():\n",
    "        categoryMap[row['subCategory']] = row['category']\n",
    "\n",
    "    data.fillna(\"\", inplace=True)\n",
    "\n",
    "    subCatArray = []\n",
    "    # first mapping the 1to1 mappings\n",
    "    for i, row in data.iterrows():\n",
    "        if (row['subCategory'] != \"\"):\n",
    "            subCatArray.append(row['subCategory'])\n",
    "        else:\n",
    "            try:\n",
    "                index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "                subCategory = maps.loc[index]['subCategory']\n",
    "                subCatArray.append(subCategory)\n",
    "            except:\n",
    "                subCatArray.append(\"\")\n",
    "\n",
    "    # then mapping all the general categories\n",
    "    data['subCategory'] = subCatArray\n",
    "    subCatArray = pd.Series(subCatArray) \n",
    "\n",
    "\n",
    "    for i, categoryRow in subCategories.iloc[::-1].iterrows():\n",
    "        indo = ((data['item'].str.contains(categoryRow['item'])) & (data['subCategory']==\"\"))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['balance']=data['credit']-data['debit']\n",
    "\n",
    "    # finally taking care of special categories with logic\n",
    "    specialCategories = subCategories[subCategories['item'].str.contains('{{')]\n",
    "    for i, categoryRow in specialCategories.iterrows():\n",
    "        itemValuePair = categoryRow['item'].replace('}}', '').split('{{')\n",
    "        indo = (data['item'].str.contains(itemValuePair[0].rstrip()) & (data['balance']==(float(itemValuePair[1]))))\n",
    "        subCatArray[indo] = categoryRow['subCategory']\n",
    "\n",
    "    data['subCategory'] = subCatArray\n",
    "    data['category'] = data['subCategory'].map(categoryMap)\n",
    "    data['year']= pd.to_datetime(data['date']).dt.year\n",
    "    data['month']= pd.to_datetime(data['date']).dt.month    \n",
    "    return data\n",
    "\n",
    "def runProcess(files):\n",
    "    newItems = listNewItems(files)\n",
    "    if(newItems['item'].count() > 0):\n",
    "        processedData = processData(newItems)   \n",
    "        dataWithoutCategory = (processedData[processedData['subCategory'] == \"\"])\n",
    "        if(len(dataWithoutCategory) == 0):\n",
    "            processedAlready = getFile('processed')\n",
    "            processedAll = pd.concat([processedData, processedAlready])\n",
    "            processedToSave = processedAll[processedColumns].sort_values(by='date', ascending=False)\n",
    "            saveDf(processedToSave, 'processed', 'data', True)\n",
    "\n",
    "            dataAll = getFile('data')\n",
    "            combinedData = pd.concat([dataAll, newItems])\n",
    "            combinedData = combinedData[oldColumns]\n",
    "            saveDf(combinedData, 'data', 'data', False)\n",
    "\n",
    "            writeToJson(processedToSave)\n",
    "\n",
    "            print(\"SAVED\")\n",
    "        else:\n",
    "            print('Found Gaps, NOT SAVED')\n",
    "            print(dataWithoutCategory[['item','date','balance']])\n",
    "    #       dataWithoutCategory[['item','date','balance']].to_csv('./processed/not_found.csv')\n",
    "    else:\n",
    "        print('no new items')\n",
    "        \n",
    "def resetToCurrentData():\n",
    "    processedData = processData(None, True)  \n",
    "    processedToSave = processedData[processedColumns].sort_values(by='date', ascending=False)\n",
    "    writeToJson(processedToSave)\n",
    "    saveDf(processedToSave, 'processed', 'data', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# files should have schema [{'name':string, 'data':pd.DataFrame}, ...]\n",
    "# files = getFiles()\n",
    "# runProcess(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resetToCurrentData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed/processed.csv'"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reset data and process from backup\n",
    "from shutil import copyfile\n",
    "copyfile(\"./backup/data.csv\", \"./processed/data.csv\")\n",
    "copyfile(\"./backup/processed.csv\", \"./processed/processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeData(hash, subCategory):\n",
    "    df = getFile(\"data\")\n",
    "    df['subCategory'].fillna(\"\",inplace=True)\n",
    "    df.fillna(value=0,inplace=True)\n",
    "    df.loc[dz[\"hash\"] == hash, \"subCategory\"] = subCategory\n",
    "    return df\n",
    "\n",
    "df = changeData(\"bec633716240d8202d8c6e815215fd20cd190bbf\", 'abbas')\n",
    "writeFile(\"data\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kzahir/anaconda3/lib/python3.6/site-packages/plaid/client.py:66: UserWarning: \n",
      "                Development is not intended for production usage.\n",
      "                Swap out url for https://production.plaid.com\n",
      "                via Client.config before switching to production\n",
      "            \n",
      "  ''')\n"
     ]
    }
   ],
   "source": [
    "client = plaid.Client(client_id = keys.PLAID_CLIENT_ID, secret=keys.PLAID_SECRET,\n",
    "                      public_key=keys.PLAID_PUBLIC_KEY, environment=keys.PLAID_ENV, api_version='2019-05-29')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def accountName(name):\n",
    "    result = None\n",
    "    if result == None:\n",
    "        result= accountList[accountList['accountId']==name]['account'].values[0]\n",
    "    if (result == \"Visa Cad\"):\n",
    "        result = \"visa\"\n",
    "    elif (result == 'Personal Line Of Credit'):\n",
    "        result = \"line\"\n",
    "    elif (result==\"Eadvantage Savings Account\"):\n",
    "        result = \"savings\"\n",
    "    elif (result==\"Tfsa Tax Advantage Savings Account\"):\n",
    "        result= \"TFSA\"\n",
    "    elif (result== \"Personal Chequing Account USD\"):\n",
    "        result = \"USD\"\n",
    "    elif (result==\"CIBC Smart Account\"):\n",
    "        result = \"debit\"\n",
    "        \n",
    "    return result\n",
    "\n",
    "def getTrans(start_date, end_date):\n",
    "        \n",
    "    response = client.Transactions.get(keys.access_token,\n",
    "                                       start_date=start_date,\n",
    "                                       end_date=end_date, count=500)\n",
    "    print(\"# Transactions found\" , response['total_transactions'])\n",
    "    \n",
    "    accountList= pd.DataFrame(columns=['account', 'subtype', 'type', 'balanceA', 'balanceC' , 'accountId'])\n",
    "    for account in response['accounts']:\n",
    "        name = account['name']\n",
    "        subtype = account['subtype']\n",
    "        type = account['type']\n",
    "        balanceA= account['balances']['available']\n",
    "        balanceC= account['balances']['current']\n",
    "        accountId= account['account_id']\n",
    "\n",
    "        b = pd.Series({'account':name,'subtype':subtype,'type':type, 'balanceA':balanceA, 'balanceC':balanceC, 'accountId':accountId})\n",
    "        accountList = accountList.append(b, ignore_index=True )\n",
    "        \n",
    "        \n",
    "    transactions = response['transactions']\n",
    "\n",
    "    # Manipulate the count and offset parameters to paginate\n",
    "    # transactions and retrieve all available data\n",
    "    while len(transactions) < response['total_transactions']:\n",
    "        response = client.Transactions.get(keysaccess_token,\n",
    "                                           start_date='2018-01-01',\n",
    "                                           end_date='2019-02-01',\n",
    "                                           offset=len(transactions)\n",
    "                                          )\n",
    "        transactions.extend(response['transactions'])\n",
    "    tx= pd.DataFrame(columns=['date', 'item', 'debit','credit', 'card'])\n",
    "    \n",
    "    for account in transactions:\n",
    "        name = account['name']\n",
    "        date = account['date']\n",
    "        debit = account['amount'] if account['amount'] >= 0 else 0;\n",
    "        credit = -account['amount'] if account['amount'] < 0 else 0;        \n",
    "        accountId= accountName(account['account_id'])\n",
    "\n",
    "        b = pd.Series({'date':date,'item':name,'debit':debit, 'credit':credit, 'card':accountId})\n",
    "        tx = tx.append(b, ignore_index=True )\n",
    "    return tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transactions found 102\n"
     ]
    }
   ],
   "source": [
    "tx = getTrans('2019-01-01', '2019-02-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles():\n",
    "    result = []\n",
    "    tx = getTrans('2019-01-01', '2019-02-01')\n",
    "    fileObject={\"name\":None, \"data\":None}\n",
    "    fileObject[\"name\"] = 'plaid'\n",
    "    fileObject[\"data\"] = tx\n",
    "    result.append(fileObject)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tx['item'].str.upper().str.contains('LON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transactions found 102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>item</th>\n",
       "      <th>debit</th>\n",
       "      <th>credit</th>\n",
       "      <th>card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>LONGO'S # 12 MAPLE, ON</td>\n",
       "      <td>105.51</td>\n",
       "      <td>0</td>\n",
       "      <td>Visa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>PREAUTHORIZED DEBIT LN # 1705943330 CIBC LOANS...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>Debit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>BUY BUY BABY #3703 WOODBRIDGE, ON</td>\n",
       "      <td>47.63</td>\n",
       "      <td>0</td>\n",
       "      <td>Visa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>DOLLARAMA # 245 MAPLE, ON</td>\n",
       "      <td>13.56</td>\n",
       "      <td>0</td>\n",
       "      <td>Visa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-31</td>\n",
       "      <td>HUSKY SWEET RIVER 6112 CONCORD, ON</td>\n",
       "      <td>37.60</td>\n",
       "      <td>0</td>\n",
       "      <td>Visa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               item   debit  \\\n",
       "0  2019-02-01                             LONGO'S # 12 MAPLE, ON  105.51   \n",
       "1  2019-02-01  PREAUTHORIZED DEBIT LN # 1705943330 CIBC LOANS...    0.04   \n",
       "2  2019-01-31                  BUY BUY BABY #3703 WOODBRIDGE, ON   47.63   \n",
       "3  2019-01-31                          DOLLARAMA # 245 MAPLE, ON   13.56   \n",
       "4  2019-01-31                 HUSKY SWEET RIVER 6112 CONCORD, ON   37.60   \n",
       "\n",
       "  credit   card  \n",
       "0      0   Visa  \n",
       "1      0  Debit  \n",
       "2      0   Visa  \n",
       "3      0   Visa  \n",
       "4      0   Visa  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files=getFiles()\n",
    "files[0]['data'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Visa', 'Debit', 'USD']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tx['card'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HASHING UTILITIES\n",
    "# hash the data file\n",
    "# old = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# old['custom'].fillna(\"\",inplace=True)\n",
    "# old.fillna(value=0,inplace=True)\n",
    "# assert old.dtypes['debit'] == 'float64'\n",
    "# assert old.dtypes['credit'] == 'float64'\n",
    "# old['hash']= hashit(old)\n",
    "# old.to_csv(f'./processed/data.csv', index=False, header=False)\n",
    "\n",
    "# hash the other files\n",
    "# for file in dlFiles:\n",
    "#     old = pd.read_csv(\"./processed/cards/\"+file, header=None,names=oldColumns)\n",
    "#     old['custom'].fillna(\"\",inplace=True)\n",
    "#     old.fillna(value=0,inplace=True)\n",
    "#     assert old.dtypes['debit'] == 'float64'\n",
    "#     assert old.dtypes['credit'] == 'float64'\n",
    "#     old['hash']= hashit(old)\n",
    "#     old.to_csv(f'./processed/{file.split(\".\")[0]}.{file.split(\".\")[1]}', index=False, header=False)\n",
    "\n",
    "# # populate account field in data\n",
    "# oldColumns=['date','item','debit','credit','custom','hash', 'account']\n",
    "\n",
    "# data = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for file in dlFiles:\n",
    "#     new = pd.read_csv(\"./processed/\"+file, header=None,names=oldColumns)\n",
    "#     for index, row in new.iterrows():\n",
    "#         if(row['hash'] in data['hash'].values):\n",
    "#             data.loc[data['hash'] == row['hash'],'account']=file.split(\".\")[0]\n",
    "            \n",
    "# data.to_csv(f'./processed/data.csv', index=False, header=False) \n",
    "\n",
    "\n",
    "# # check for duplicate fields\n",
    "\n",
    "# data2 = pd.read_csv(\"./processed/data.csv\", header=None,names=oldColumns)\n",
    "# for index, row in data2.iterrows():\n",
    "#     if(data2[data2['hash']==row['hash']].count()['hash']>2):\n",
    "#         print(row['hash'], row['item'], row['debit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # PROCESS BENCH MARKING\n",
    "# # TESTING TWO FILES\n",
    "# old = pd.read_csv(\"./processed/output-old.csv\", index_col=False)\n",
    "# new = pd.read_csv(\"./processed/processed.csv\", index_col=False)\n",
    "# old.sort_values(by=['date','debit'], inplace=True)\n",
    "# new.sort_values(by=['date','debit'], inplace=True)\n",
    "# old.reset_index( drop=True,inplace=True)\n",
    "# new.reset_index(drop=True,inplace=True)\n",
    "# ineq = []\n",
    "# for i, row in old.iterrows():\n",
    "#     if (row['subCategory'] != new.loc[i]['subCategory']):\n",
    "#         if(row['item']==\"TIGERDIRECT.CA MARKHAM, ON\"):\n",
    "#             pass\n",
    "#         elif('WINNERSHOMESENSE' in row['item']):\n",
    "#             pass\n",
    "#         else:\n",
    "#             pass\n",
    "# #             print(f\"{row['subCategory']}, {new.loc[i]['subCategory']} - {row['balance']} {row['item']}{new.loc[i]['item']}\")\n",
    "            \n",
    "\n",
    "# y = new.groupby('subCategory').sum()['balance']\n",
    "# x = old.groupby('subCategory').sum()['balance']\n",
    "# for i,value in enumerate(x):\n",
    "#     if(value != y[i]):\n",
    "#         print(value, y[i])\n",
    "\n",
    "# # one to one map benchmarking\n",
    "# def try1():    \n",
    "#     def check1to1(x):\n",
    "#         try:\n",
    "#             index = pd.Index(maps['item']).get_loc(x.rstrip())   \n",
    "#             return maps.loc[index]['subCategory']\n",
    "#         except:\n",
    "#             return None\n",
    "\n",
    "#     data['subCategory'] = data['subCategory'].combine_first(data['item'].apply(check1to1))\n",
    "    \n",
    "# def try2():\n",
    "#     data.fillna(\"\", inplace=True)\n",
    "#     subCatArray = []\n",
    "#     # first mapping the 1to1 mappings\n",
    "#     for i, row in data.iterrows():\n",
    "#         if (row['subCategory'] != \"\"):\n",
    "#             subCatArray.append(row['subCategory'])\n",
    "#         else:\n",
    "#             try:\n",
    "#                 index = pd.Index(maps['item']).get_loc(row['item'].rstrip())\n",
    "#                 subCategory = maps.loc[index]['subCategory']\n",
    "#                 subCatArray.append(subCategory)\n",
    "#             except:\n",
    "#                 subCatArray.append(\"\")\n",
    "#     data['subCategory'] = subCatArray\n",
    "# %timeit try1()\n",
    "# %timeit try2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from process import processData, getFile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processData(None, True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = getFile('data')\n",
    "pr = getFile('processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
